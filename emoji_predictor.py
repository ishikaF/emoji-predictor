# -*- coding: utf-8 -*-
"""Group1_Tweet_Emoji_Prediction_w_demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1om6ocpv-UBceEt6ef_iUFQBgnyBi1r3R

# Group 1 - Twitter Emoji Prediction Deep Learning Project

Group Members:
- Safoora Akrami
- Lok Yin Wong
- Lily Gharacheh
- Shashi Singh
- Vrushank Sharma
- Avinash Sudireddy
- Ishika Fatwani

## Step 1: Setup & Project Overview

### Dataset Source:
We are using the [Twitter Emoji Prediction dataset](https://www.kaggle.com/api/v1/datasets/download/hariharasudhanas/twitter-emoji-prediction) from Kaggle. It contains tweets labeled with emojis, used to train models for multi-class emoji prediction.

Objective: Build a deep learning model to predict the most appropriate emoji for a tweet.

Problem Type: Multi-class classification problem with 20 emoji categories.

Why This Matters:
 Improves social media analytics and sentiment understanding
 Real-world application of NLP and deep learning
"""

!curl -L -o ./twitter-emoji-prediction.zip https://www.kaggle.com/api/v1/datasets/download/hariharasudhanas/twitter-emoji-prediction
!unzip -q twitter-emoji-prediction.zip

"""###  Library Installation
We install the latest versions of:
- `transformers`: for working with pretrained models like BERT.
- `keras`: for deep learning model building.
"""

!pip install -q --upgrade transformers
!pip install -q --upgrade keras

"""###  Compatibility Settings
We explicitly set the environment variable `TF_USE_LEGACY_KERAS=1` to ensure compatibility between Keras and Hugging Face Transformers.
"""

import os
os.environ['TF_USE_LEGACY_KERAS'] = '1'

"""###  Library Imports

We begin by importing essential libraries:

- **NumPy (`np`)**: For array and numerical computations.
- **Pandas (`pd`)**: For loading and managing tabular data.
- **TensorFlow (`tf`)**: Main framework used for building and training neural networks.
- **Matplotlib (`plt`)**: For plotting evaluation metrics like accuracy and loss.

"""

# Import necessary libraries
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from google.colab import files

"""###  Check for GPU Availability

We check if a GPU is available using TensorFlow. If not, we fall back to using the CPU.

"""

# Check if GPU is present

try:
  device_name = tf.test.gpu_device_name()
  if device_name != '/device:GPU:0':
      raise SystemError('GPU device not found')
  print('Found GPU at: {}'.format(device_name))
except:
  device_name = '/cpu:0'
  print('No GPU found. Using CPU instead.')

print(f"Using device {device_name}")

"""## Step 2: Load and Explore the Dataset

Data Info:
- ~70,000 labelled Tweets
- Each Tweet is labelled with 1 of 20 Emojis
- Input: Tweet text
- Target: Emoji class (label)



We load the training data from `Train.csv`, keeping only the relevant columns: tweet text and its corresponding label and then rename its columns to lowercase.

"""

# Load train set
train_df = pd.read_csv('Train.csv', usecols=[1, 2])

train_df.columns = train_df.columns.str.lower()
train_df.head()

"""**Emoji Label Mapping**
Reads only columns 1 and 2 (ignoring column 0)
Assigns custom column names: "emoji" and "label"

To interpret model predictions, we load `Mapping.csv`, which maps numeric labels to actual emojis.
"""

# Load emoji mapping
mapping_df = pd.read_csv("Mapping.csv", usecols=[1, 2], names=["emoji", "label"], header=0)

mapping_df.head()

"""Number of entries: 70,000
Columns: text (tweets) and label (emoji class)
Data types: object for text, int64 for labels

Confirms there are no missing values, Finds and counts duplicate rows (there are 69), Checks for missing (null) values (there are none)
"""

train_df.info()
print("Duplicated rows", train_df.duplicated().sum())
print("Null rows", train_df.isna().sum())

print(train_df.value_counts("label"))
train_df["label"].value_counts().plot(kind="bar")

"""## Step 3: Data Preprocessing

### Text Cleaning

In the data preprocessing part, we first drop the duplicated rows we identified before.

Then, we clean the text by removing mentions, URLs and hashtags. After that, the text is converted to lower case and stripped.
"""

# Clean text
import re
def clean_text(text):
  """Cleans a given text string by removing mentions, URLs, and hashtags.

  This function performs the following operations:
  - Removes Twitter-style mentions (e.g., @username)
  - Removes URLs (e.g., http://example.com)
  - Removes hashtags (e.g., #topic)
  - Converts the text to lowercase
  - Strips leading and trailing whitespace

  Args:
      text (str): The input text string to clean.

  Returns:
      str: The cleaned text.
  """
  text = re.sub(r"@\w+", "", text)
  text = re.sub(r"http\S+", "", text)
  text = re.sub(r"#\w+", "", text)
  return text.lower().strip()

train_df.drop_duplicates(inplace=True)
train_df["clean_text"] = train_df["text"].apply(clean_text)
train_df.head()

"""### Distribution Balancing

Since the distribution of labels is imbalance. We decided to keep only the top 9 labels with largest amoung of samples.

The script performs the following

1. Filters the dataset to include only the specified target_labels

2. Downsamples each class to match the smallest class count

3. Shuffles the resulting balanced dataset

4. Visualizes the new class distribution with a bar chart
"""

target_labels = [9, 2, 3, 7, 15, 13, 16, 17, 1]

filtered_df = train_df[train_df['label'].isin(target_labels)]

min_count = filtered_df['label'].value_counts().min()

balanced_df = pd.concat([
    group.sample(n=min_count, random_state=1234)
    for _, group in filtered_df.groupby('label')
])

balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)

balanced_df["label"].value_counts().plot(kind="bar")

"""## Step 4: Train / Validation Split

Splits cleaned and labeled text data into training, validation, and test sets using stratified sampling.

The script performs the following:
1. Extracts texts and labels from a preprocessed DataFrame to a numpy array.
2. Maps string labels to integer indices.
3. Splits the dataset into training, validation, and test sets using a fixed seed and stratification.

"""

from sklearn.model_selection import train_test_split

seed = 1234

texts = balanced_df['clean_text'].to_numpy()
labels = balanced_df['label'].to_numpy()

original_labels = sorted(list(set(labels)))
label2idx = {label: idx for idx, label in enumerate(original_labels)}

# Map labels
labels_mapped = np.array([label2idx[label] for label in labels])

train_test_texts, val_texts, train_test_labels, val_labels = train_test_split(
    texts, labels_mapped, test_size=0.2, random_state=seed, stratify=labels
)

train_texts, test_texts, train_labels, test_labels = train_test_split(
    train_test_texts, train_test_labels, test_size=0.1, random_state=seed, stratify=train_test_labels
)

print(f"Train size: {len(train_texts)}")
print(f"Test size: {len(test_texts)}")
print(f"Validation size: {len(val_texts)}")

"""## Step 5: Encode texts

We chose to encode the texts in two ways - BertTokenizer and Vectorizer with gloVe Embeddings. This is the first method.

The script performs the following
1. Load pretrained Bert tokenizer
2. Define a function that tokenize the texts and create a tf dataset for BERT-based models
3. Call tokenize for train, val, and test set respectively and batch them
"""

from transformers import BertTokenizerFast

bert_tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')

MAX_LEN = 100
BATCH_SIZE = 32

def tokenize(texts, labels, max_len=100):
  """Tokenizes input texts and creates a TensorFlow dataset for BERT-based models.

  This function uses a pretrained BERT tokenizer to convert raw text inputs into
  token ID and attention mask pairs. The resulting tokenized inputs and associated
  labels are formatted as a `tf.data.Dataset`.

  Args:
      texts (list or np.ndarray): A list or array of input text strings to tokenize.
      labels (list or np.ndarray): Corresponding labels for each input text.
      max_len (int, optional): The maximum sequence length for tokenization. Defaults to 100.

  Returns:
      tf.data.Dataset: A dataset of tokenized inputs and labels, where each element is a tuple:
          ({
              "input_ids": tf.Tensor,
              "attention_mask": tf.Tensor
          }, label)
  """
  encodings = bert_tokenizer(
      texts,
      truncation=True,
      padding='max_length',
      max_length=max_len,
  )
  return tf.data.Dataset.from_tensor_slices(({
      "input_ids": encodings["input_ids"],
      "attention_mask": encodings["attention_mask"]
    }, labels))

# Create train and val datasets
bert_train_ds = tokenize(train_texts.tolist(), train_labels, max_len=MAX_LEN).cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
bert_test_ds = tokenize(test_texts.tolist(), test_labels, max_len=MAX_LEN).cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
bert_val_ds = tokenize(val_texts.tolist(), val_labels, max_len=MAX_LEN).cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

"""The following script save the tokenizer and clean text function into a class instance and dump into a pickle object. This makes preprocessing easier in Streamlit."""

import pickle
import re

class TextProcessor:
  """Processes and tokenizes text data for BERT-style models.

  This class provides methods for cleaning raw text and converting it
  into tokenized input suitable for transformer-based models such as BERT.

  Attributes:
      tokenizer: A Hugging Face tokenizer instance used for tokenization.
  """
  def __init__(self, tokenizer):
      """Initializes the TextProcessor with a given tokenizer.

      Args:
          tokenizer: A Hugging Face tokenizer (e.g., BertTokenizer or BertTokenizerFast).
      """
      self.tokenizer = tokenizer

  def clean_text(self, text):
      """Cleans the input text by removing mentions, URLs, and hashtags.

      Applies regex-based cleaning to strip out unwanted patterns and normalizes the text.

      Args:
          text (str): The raw input string.

      Returns:
          str: A cleaned and lowercased version of the input text.
      """
      text = re.sub(r"@\w+", "", text)
      text = re.sub(r"http\S+", "", text)
      text = re.sub(r"#\w+", "", text)
      return text.lower().strip()

  def __call__(self, text, max_len=100):
      """Cleans and tokenizes the input text, returning a TensorFlow dataset.

      This method makes the class callable. It performs cleaning and tokenization,
      and returns a dataset with input IDs and attention masks.

      Args:
          text (str): The input text string to process.
          max_len (int, optional): Maximum sequence length for padding/truncation. Defaults to 100.

      Returns:
          tf.data.Dataset: A dataset with a single element containing:
              {
                  "input_ids": tf.Tensor,
                  "attention_mask": tf.Tensor
              }
      """
      cleaned_text = self.clean_text(text)
      encodings = self.tokenizer(
          [cleaned_text],  # Wrap in a list as tokenizer expects list of strings
          truncation=True,
          padding='max_length',
          max_length=max_len,
          return_tensors='tf'
      )

      return tf.data.Dataset.from_tensor_slices({
          "input_ids": encodings["input_ids"],
          "attention_mask": encodings["attention_mask"]
        })

text_processor = TextProcessor(bert_tokenizer)

with open('bert_text_processor.pkl', 'wb') as f:
    pickle.dump(text_processor, f)

files.download("bert_text_processor.pkl")

"""## Step 6: Build Model

The following section lists various models to approach this problem.

The section is divided into two main stream of models - BERT-based and gloVe embedding-based.

### Utils Functions
Computes balanced class weights for use in model training.

This script calculates weights to address class imbalance in the training data
using the `balanced` mode from scikit-learn. The resulting weights can be used
during model training to give higher importance to underrepresented classes.
"""

from sklearn.utils.class_weight import compute_class_weight
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(train_labels),
    y=train_labels
)

class_weight_dict = {i: w for i, w in enumerate(class_weights)}
print("Class Weights:", class_weight_dict)

num_classes = len(set(labels))

def plot_history(history):
  """Plots training and validation accuracy and loss from a Keras History object.

  This function visualizes the accuracy and loss over training epochs using data
  from a Keras `History` object returned by `model.fit()`. It produces two subplots:
  one for accuracy and one for loss, each showing training and validation curves.

  Args:
      history (tf.keras.callbacks.History): The history object returned by model training.

  Raises:
      ValueError: If the history object is None or does not contain expected keys.

  Example:
      history = model.fit(...)
      plot_history(history)
  """

  if not history:
      print("No training history provided.")
      return

  acc = history.history.get('accuracy')
  val_acc = history.history.get('val_accuracy')
  loss = history.history.get('loss')
  val_loss = history.history.get('val_loss')
  epochs = range(1, len(acc) + 1)

  plt.figure(figsize=(14, 5))

  # Accuracy
  plt.subplot(1, 2, 1)
  plt.plot(epochs, acc, 'bo-', label='Training Accuracy')
  plt.plot(epochs, val_acc, 'ro-', label='Validation Accuracy')
  plt.title('Training and Validation Accuracy')
  plt.xlabel('Epochs')
  plt.ylabel('Accuracy')
  plt.legend()

  # Loss
  plt.subplot(1, 2, 2)
  plt.plot(epochs, loss, 'bo-', label='Training Loss')
  plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')
  plt.title('Training and Validation Loss')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.legend()

  plt.tight_layout()
  plt.show()

class DotDict:
  """Converts a dictionary to an object with dot-accessible attributes."""
  def __init__(self, dictionary):
    for key, value in dictionary.items():
      setattr(self, key, value)

def merge_histories(h1, h2):
  """Merges the history objects of two model training runs.

  This function assumes that both `h1` and `h2` are Keras History objects
  and merges their metrics by concatenating the lists.

  Args:
      h1 (tf.keras.callbacks.History): History from the first training phase.
      h2 (tf.keras.callbacks.History): History from the second training phase.

  Returns:
      DotDict: A dot-accessible dictionary with the merged 'history' key.
  """
  merged = {}
  for key in h1.history:
      merged[key] = h1.history[key] + h2.history[key]
  return DotDict({"history": merged})

"""### Bert transformer

RAM: 7.2GB
GPU RAM: 4.0GB

**BERT (Bidirectional Encoder Representations from Transformers)** is a powerful pre-trained language model developed by Google that understands language context by reading text bidirectionally. In this project, we explored multiple ways of leveraging BERT for text classification to improve accuracy, generalization, and feature extraction.
"""

from tensorflow.keras import backend as K
K.clear_session()

from tensorflow.keras.mixed_precision import set_global_policy
set_global_policy('mixed_float16')

"""#### Fine-Tuned BERT (TFBertForSequenceClassification)

**Why**: This approach adapts the entire BERT model, including its internal attention weights, to our specific classification task.


**Advantage**: Since BERT was trained on a massive corpus (e.g., Wikipedia and BooksCorpus), fine-tuning allows us to benefit from its language understanding while customizing it to our dataset.


**Use Case**: Ideal for quick and effective performance with minimal custom architecture.


"""

from transformers import TFBertForSequenceClassification

with tf.device(device_name):
  bert_fine_tune_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)

  bert_fine_tune_model.compile(
      optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),
      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
      metrics=['accuracy']
  )

  bert_fine_tune_model.summary()

  history = bert_fine_tune_model.fit(
    bert_train_ds,
    validation_data=bert_val_ds,
    class_weight=class_weight_dict,
    batch_size=BATCH_SIZE,
    epochs=4,
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)]
  )

  plot_history(history)
  bert_fine_tune_model.save_pretrained("fine_tuned_bert")

"""Accuracy Trend:
- Training Accuracy increased rapidly, reaching ~60% by epoch 3
- Validation Accuracy stayed flat around 38%, showing no improvement after epoch 1
- Indicates overfitting — model learns the training data well but doesn’t generalize

Loss Trend:
- Training Loss decreased significantly from ~1.9 to ~1.2
- Validation Loss slightly increased, ending above 1.9
- Confirms that the model is memorizing the training data rather than learning meaningful patterns

Despite high training performance, the gap between training and validation shows poor generalization


The model is too complex for the current data size, or the data itself has label noise or ambiguity




"""

!zip -r fine_tuned_bert.zip fine_tuned_bert
files.download("fine_tuned_bert.zip")

"""#### Bert with Classification Layer

RAM: 7.1GB
GPU RAM: 4.1GB

**BERT + Custom Classification Layers (CLS Token with Dense Layers)**

Why: Instead of using BERT's default classification head, this approach extracts the [CLS] token embedding and passes it through a custom multi-layer neural network.

Advantage:

More flexibility in architecture design (e.g., adding batch normalization, regularization, and dropout).

Better control over model capacity and regularization, especially for domain-specific data.

Use Case: When you want to fine-tune BERT but also apply deep learning best practices to tailor the classifier head.
"""

from transformers import TFBertModel
from tensorflow.keras.layers import Input, Dense, Dropout, Lambda, BatchNormalization, Layer
from tensorflow.keras.models import Model

class BertCLS(Layer):
  """Custom Keras layer to extract the [CLS] token embedding from a BERT model.

  This layer wraps a pretrained BERT model and returns the output corresponding to the
  [CLS] token, which is typically used for classification tasks.

  Attributes:
      bert: A pretrained Hugging Face BERT model (e.g., TFBertModel).
  """
  def __init__(self, bert_model, **kwargs):
      super().__init__(**kwargs)
      self.bert = bert_model

  def call(self, inputs):
      input_ids, attention_mask = inputs
      outputs = self.bert(input_ids, attention_mask=attention_mask)
      return outputs.last_hidden_state[:, 0, :]  # [CLS] token output

def get_bert_with_class_layer_model(num_classes):
  """Builds a BERT-based text classification model with custom classification layers.

  This function constructs a TensorFlow Keras model that uses a pretrained BERT model
  as a base and adds custom fully connected layers for classification. The [CLS] token
  embedding is extracted via a custom `BertCLS` layer.

  Architecture:
    - BERT model (`bert-base-uncased`) for contextual encoding
    - [CLS] token extraction via `BertCLS`
    - Fully connected layers with dropout, batch normalization, and ReLU activations
    - Final softmax output layer for multi-class classification

  Args:
      num_classes (int): Number of target classes for classification.

  Returns:
      tf.keras.Model: A compiled Keras model ready for training.

  Example:
      model = get_bert_with_class_layer_model(num_classes=5)
      model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
  """
  bert = TFBertModel.from_pretrained("bert-base-uncased")
  input_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_ids')
  attention_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name='attention_mask')

  x = BertCLS(bert)([input_ids, attention_mask])

  x = Dropout(0.3)(x)
  x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)
  x = BatchNormalization()(x)
  x = Dropout(0.3)(x)
  x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)
  x = BatchNormalization()(x)
  x = Dense(128, activation='relu')(x)
  x = Dropout(0.3)(x)

  output = Dense(num_classes, activation='softmax')(x)

  model = Model(inputs=[input_ids, attention_mask], outputs=output)
  return model

"""Trains a BERT-based classification model using a custom classifier head.

This script compiles and trains a BERT model with learning rate scheduling,
early stopping, and learning rate reduction on plateau. It also plots training history
and saves the trained model weights.
"""

num_train_samples = len(train_labels)
epochs = 10
total_steps = (num_train_samples // BATCH_SIZE) * epochs

with tf.device(device_name):
  bert_with_class_layer_model = get_bert_with_class_layer_model(num_classes)

  lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
    initial_learning_rate=4e-5,
    decay_steps=total_steps,
    end_learning_rate=1e-6
  )

  bert_with_class_layer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
  bert_with_class_layer_model.summary()

  history = bert_with_class_layer_model.fit(
    bert_train_ds,
    validation_data=bert_val_ds,
    class_weight=class_weight_dict,
    epochs=epochs,
    batch_size=BATCH_SIZE,
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=2,  restore_best_weights=True), tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)]
  )
  plot_history(history)
  bert_with_class_layer_model.save_weights("bert_with_class_layer_model.weights.h5")

"""Accuracy Trend:
- Training Accuracy increased steadily, reaching ~63% by epoch 10
- Validation Accuracy improved early on, peaking around 38%, then plateaued
- This indicates the model learned well, but generalization stopped improving after epoch 5

Loss Trend:
- Training Loss dropped consistently from ~12 to ~7.5
- Validation Loss decreased at first, then stabilized around 8.8, showing early convergence
- No major overfitting, but also no further gain in validation performance

Model is learning effectively, but validation performance plateaus early


"""

files.download('bert_with_class_layer_model.weights.h5')

"""#### Bidirectional LSTM with Bert Embeddings
RAM: 7.1GB GPU RAM: 4.1GB

**BERT + Bidirectional LSTM (BERT as Feature Extractor)**

Why: This setup uses BERT to generate contextualized token embeddings, then processes them using a BiLSTM layer to capture sequential dependencies.

Advantage:

- Sequential modeling: LSTM captures positional and temporal patterns, which BERT doesn't explicitly model.

- Fine-grained context: BiLSTM can enhance downstream task performance by re-interpreting BERT's embeddings with temporal dynamics.

Use Case: Useful for tasks where order and flow of tokens are crucial (e.g., sentiment analysis, complex linguistic structures).
"""

from transformers import TFBertModel
from tensorflow.keras.layers import Input, Bidirectional, LSTM, Dense, Dropout, Lambda, BatchNormalization
from tensorflow.keras.models import Model

class Bert(Layer):
  """Custom Keras layer to extract sequence output from a BERT model.

  This layer wraps a pretrained Hugging Face BERT model and returns the
  full sequence output (`last_hidden_state`), which includes contextual
  embeddings for each token in the input.

  Attributes:
      bert: A pretrained Hugging Face BERT model (e.g., TFBertModel).
  """
  def __init__(self, bert_model, **kwargs):
      super().__init__(name="bert_sequence_output", **kwargs)
      self.bert = bert_model

  def call(self, inputs):
      input_ids, attention_mask = inputs
      outputs = self.bert(input_ids, attention_mask=attention_mask)
      return outputs.last_hidden_state

def get_bidirectional_lstm_bert_model(num_labels):
  """Builds a BERT + Bidirectional LSTM model for text classification.

  This model uses a frozen BERT encoder to extract token-level embeddings and passes
  them through a Bidirectional LSTM followed by fully connected layers for
  classification.

  Architecture:
      - Pretrained BERT (frozen weights)
      - Bidirectional LSTM (128 units in each direction)
      - Dense layers with dropout and batch normalization
      - Softmax output for multi-class classification

  Args:
      num_labels (int): Number of output classes.

  Returns:
      tf.keras.Model: A compiled Keras model ready for training.
  """
  # Load base BERT model (no classification head)
  bert_model = TFBertModel.from_pretrained('bert-base-uncased')
  bert_model.trainable = False

  # Input layers
  input_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_ids')
  attention_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name='attention_mask')

  x = Bert(bert_model)([input_ids, attention_mask])  # shape: (batch_size, MAX_LEN, 768)

  x = Bidirectional(LSTM(128, return_sequences=False))(x)  # shape: (batch_size, 256)

  # Dense + Dropout + BatchNorm head
  x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)
  x = BatchNormalization()(x)
  x = Dropout(0.3)(x)

  x = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)
  x = BatchNormalization()(x)
  x = Dropout(0.3)(x)

  output = Dense(num_labels, activation='softmax')(x)

  return Model(inputs=[input_ids, attention_mask], outputs=output)

"""Trains a two-phase BiLSTM-BERT model with optional fine-tuning.

This script:
  - Initializes a frozen BERT + BiLSTM model.
  - Trains it with BERT frozen for 3 epochs.
  - Unfreezes BERT and continues fine-tuning.
  - Merges training histories.
  - Plots performance and saves final weights.
"""

with tf.device(device_name):

  bert_bi_lstm_model = get_bidirectional_lstm_bert_model(num_classes)

  bert_bi_lstm_model.compile(
      optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),
      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
      metrics=['accuracy']
  )
  bert_bi_lstm_model.summary()

  history1 = bert_bi_lstm_model.fit(
    bert_train_ds,
    validation_data=bert_val_ds,
    class_weight=class_weight_dict,
    epochs=3,
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)]
  )

  bert_bi_lstm_model.get_layer("bert_sequence_output").bert.trainable = True

  bert_bi_lstm_model.compile(
      optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),
      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
      metrics=['accuracy']
  )
  bert_bi_lstm_model.summary()

  history2 = bert_bi_lstm_model.fit(
    bert_train_ds,
    validation_data=bert_val_ds,
    class_weight=class_weight_dict,
    initial_epoch=3,
    epochs=10,
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)]
  )

  history = merge_histories(history1, history2)

  plot_history(history)
  bert_bi_lstm_model.save_weights("bidirectional_lstm_bert_model.weights.h5")

"""Accuracy trend:

- Training accuracy is increasing rapidly, indicating the model is learning from training data effectively.

- Validation accuracy peaks at Epoch 7, then flattens or declines slightly, even as training accuracy continues improving.

- This suggests the model starts overfitting after Epoch 7.


Loss trend:

- Training loss continues decreasing sharply.

- Validation loss plateaus after Epoch 8, despite training loss dropping further → another clear sign of overfitting.
"""

files.download("bidirectionl_lstm_bert_model.weights.h5")

"""### Glove Embeddings

**GloVe (Global Vectors for Word Representation)** is a widely used pre-trained word embedding model that captures the semantic meaning of words based on their global co-occurrence statistics in large text corpora. In this project, we use GloVe embeddings as the foundation for several neural models due to the following reasons:

1. Leverage Pre-trained Semantic Knowledge
  - GloVe is trained on massive text datasets (e.g., Wikipedia, Common Crawl).
  - Words that appear in similar contexts have similar vectors, allowing the model to generalize better with limited training data.
2. Improve Convergence and Stability
  - Pre-trained embeddings reduce the number of parameters the model must learn from scratch.
  - Leads to faster convergence, more stable training, and often better performance than training embeddings from random initialization.
3. Handle Small/Medium Datasets Better
  - When working with limited labeled data, training high-quality word embeddings from scratch is impractical.
  - GloVe provides a strong initialization, helping the model learn meaningful representations without overfitting.
4. Freeze to Reduce Overfitting
  - Preserve their semantic structure.
  - Reduce the number of trainable parameters.
"""

!wget https://nlp.stanford.edu/data/glove.twitter.27B.zip
 !unzip -q glove.twitter.27B.zip

def load_glove_embeddings(glove_file_path):
    """Loads GloVe word embeddings from a file into a dictionary.

    This function reads a GloVe-formatted file where each line contains a word
    followed by its embedding vector components. It constructs a dictionary mapping
    each word to its corresponding embedding vector.

    Args:
        glove_file_path (str): Path to the GloVe embedding file.

    Returns:
        dict: A dictionary where keys are words (str) and values are NumPy arrays
              representing the embedding vectors (dtype=float32).

    Example:
        embeddings_index = load_glove_embeddings("glove.twitter.27B.100d.txt")
        print(embeddings_index["happy"])  # Output: np.ndarray of shape (100,)
    """
    embeddings_index = {}
    with open(glove_file_path, encoding='utf-8') as f:
        for line in f:
            values = line.strip().split()
            word = values[0]
            coefs = np.asarray(values[1:], dtype='float32')
            embeddings_index[word] = coefs
    return embeddings_index

glove_path = 'glove.twitter.27B.100d.txt'
embedding_dim = 100
embeddings_index = load_glove_embeddings(glove_path)

"""Creates and fits a TextVectorization layer on the training text data.

This script builds a Keras `TextVectorization` layer to convert raw text
into integer token sequences. It also returns the vocabulary and word index mapping.
"""

from tensorflow.keras.layers import TextVectorization

max_tokens = 30000

vectorizer = TextVectorization(
    max_tokens=max_tokens,
    output_sequence_length=embedding_dim,
    output_mode='int'
)

# Fit on training text
vectorizer.adapt(train_texts)
vocab = vectorizer.get_vocabulary()
word_index = {word: idx for idx, word in enumerate(vocab)}

"""Builds an embedding matrix for a Keras Embedding layer using pre-trained embeddings.

This script creates a matrix where each row corresponds to a word index from the
vocabulary and contains the GloVe embedding vector for that word. Words not found in the `embeddings_index` are initialized with zeros.
"""

num_words = len(vocab)
embedding_matrix = np.zeros((num_words, embedding_dim))

for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

import pickle
import re

class GloveTextProcessor:
    """Cleans raw text and wraps it into a TensorFlow dataset for GloVe-based models.

    This class is designed to preprocess a single input text string by removing mentions,
    URLs, and hashtags, then converting it into a lowercased and stripped version.
    The result is wrapped in a `tf.data.Dataset` to be compatible with downstream pipelines.
    """
    def clean_text(self, text):
      text = re.sub(r"@\w+", "", text)
      text = re.sub(r"http\S+", "", text)
      text = re.sub(r"#\w+", "", text)
      return text.lower().strip()

    def __call__(self, text):
        cleaned_text = self.clean_text(text)
        return tf.data.Dataset.from_tensor_slices(tf.constant([cleaned_text], dtype=tf.string))

with open('glove_text_processor.pkl', 'wb') as f:
    pickle.dump(GloveTextProcessor(), f)

files.download("glove_text_processor.pkl")

glove_train_ds = tf.data.Dataset.from_tensor_slices((train_texts, train_labels)).batch(32)
glove_test_ds = tf.data.Dataset.from_tensor_slices((test_texts, test_labels)).batch(32)
glove_val_ds = tf.data.Dataset.from_tensor_slices((val_texts, val_labels)).batch(32)

"""#### Bidirectional LSTM

RAM: 7.9GB
GPU RAM: 0.4GB

**Why Use Bidirectional LSTM for Emoji Prediction?

Tweets are short, informal, and highly context-dependent. A regular LSTM reads text left to right and may miss important backward dependencies.

BiLSTM reads the text in both directions, capturing context from both preceding and following words.

Example: In “I didn’t hate it”, the word “didn’t” modifies “hate”. BiLSTM helps understand this reversal better than unidirectional LSTM.

When combined with embeddings like GloVe, BiLSTM can map semantic similarity while modeling complex relationships in the text.

It builds richer representations by combining word meaning (via GloVe) and sequence structure (via BiLSTM).


"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout, BatchNormalization

def get_bidirectional_lstm_glove_model(num_labels):
  """Builds a bidirectional LSTM model using pretrained GloVe embeddings for text classification.

  This model performs the following:
    - Accepts raw text input (as string).
    - Tokenizes and vectorizes text using a predefined `TextVectorization` layer.
    - Embeds tokens using pretrained GloVe vectors (frozen).
    - Processes token embeddings through stacked Bidirectional LSTM layers.
    - Applies batch normalization, dropout, and dense layers for classification.

  Assumes the following global variables are predefined:
    - `vectorizer`: A Keras `TextVectorization` layer fitted on training data.
    - `num_words`: Size of the vocabulary.
    - `embedding_dim`: Dimensionality of GloVe embeddings.
    - `embedding_matrix`: Precomputed NumPy matrix of GloVe vectors (shape: [num_words, embedding_dim]).

  Args:
      num_labels (int): Number of target classes for classification.

  Returns:
      tf.keras.Model: A compiled Keras Functional API model ready for training.

  Example:
      model = get_bidirectional_lstm_glove_model(num_labels=5)
      model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
  """
  # Input is raw text
  text_input = Input(shape=(1,), dtype=tf.string, name='text')
  x = vectorizer(text_input)
  x = Embedding(
      name="embedding",
      input_dim=num_words,
      output_dim=embedding_dim,
      weights=[embedding_matrix],
      trainable=False
  )(x)

  x = Bidirectional(LSTM(128, return_sequences=True))(x)
  x = Bidirectional(LSTM(64, dropout=0.3))(x)
  x = BatchNormalization()(x)
  x = Dropout(0.3)(x)
  x = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-3))(x)
  x = Dropout(0.2)(x)
  x = Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-3))(x)
  output = Dense(num_labels, activation='softmax')(x)

  return Model(inputs=text_input, outputs=output)

with tf.device(device_name):
  glove_bi_lstm_model = get_bidirectional_lstm_glove_model(num_classes)

  glove_bi_lstm_model.compile(
      optimizer=tf.keras.optimizers.Adam(learning_rate=2e-3, clipnorm=1.0),
      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
      metrics=['accuracy']
  )

  glove_bi_lstm_model.summary()

  history1 = glove_bi_lstm_model.fit(
    glove_train_ds,
    validation_data=glove_val_ds,
    class_weight=class_weight_dict,
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, verbose=1, restore_best_weights=True), tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)],
    batch_size=32,
    epochs=2
  )

  glove_bi_lstm_model.get_layer("embedding").trainable = True

  glove_bi_lstm_model.compile(
      optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, clipnorm=1.0),
      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
      metrics=['accuracy']
  )

  glove_bi_lstm_model.summary()

  history2 = glove_bi_lstm_model.fit(
    glove_train_ds,
    validation_data=glove_val_ds,
    class_weight=class_weight_dict,
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, verbose=1, restore_best_weights=True), tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)],
    batch_size=32,
    initial_epoch=2,
    epochs=7
  )
  history = merge_histories(history1, history2)
  plot_history(history)
  glove_bi_lstm_model.save("bidirectional_lstm_glove_model.keras")

"""Accuracy Trend:

- Training accuracy improves steadily from 21.2% to 45.5% across 7 epochs.

- Validation accuracy rises consistently from 23.7% to 32.9%, with the best performance achieved at the final epoch (Epoch 7).

- While the gap between training and validation accuracy increases over time, the validation accuracy still trends upward, suggesting moderate overfitting but continued generalization.

- Overall, the model shows healthy learning progression, especially after unfreezing embeddings and lowering the learning rate.


Loss Trend:

- Training loss decreases continuously from 2.18 → 1.60, indicating effective learning.

- Validation loss drops from 2.10 to 1.91 by Epoch 6, suggesting improved generalization.

- A slight increase in validation loss at Epoch 7 (to 1.93) coincides with the highest validation accuracy, indicating a possible trade-off between prediction sharpness and misclassification penalties.

- No early stopping was triggered, and the final weights from Epoch 7 reflect the peak performance on validation accuracy.
"""

files.download("bidirectional_lstm_glove_model.keras")

"""#### LSTM + Attention

"""

class AttentionLayer(tf.keras.layers.Layer):
    """Applies attention mechanism over temporal inputs (e.g., LSTM outputs).

    This layer computes attention scores over each time step of the input sequence,
    weights the sequence accordingly, and returns a context vector as the weighted sum.

    The attention mechanism is defined as:
        score = tanh(inputs)
        weights = softmax(score, axis=1)
        context = sum(weights * inputs, axis=1)

    Inputs:
        A 3D tensor of shape (batch_size, time_steps, hidden_size)

    Output:
        A 2D tensor of shape (batch_size, hidden_size) representing the attention-weighted context.

    Example:
        lstm_output = Bidirectional(LSTM(..., return_sequences=True))(x)
        context_vector = AttentionLayer()(lstm_output)
    """
    def __init__(self):
        super(AttentionLayer, self).__init__()

    def call(self, inputs):
        score = tf.nn.tanh(inputs)
        weights = tf.nn.softmax(score, axis=1)
        context = weights * inputs
        context = tf.reduce_sum(context, axis=1)
        return context

from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.models import Sequential

def get_lstm_attention_glove_model(num_labels):
  """Builds an LSTM-based text classification model with attention and GloVe embeddings.

  This model performs the following operations:
    - Accepts raw text input as strings.
    - Tokenizes the input using a predefined `TextVectorization` layer.
    - Converts token sequences into dense vectors using pretrained GloVe embeddings.
    - Processes the sequences through an LSTM layer.
    - Applies an attention mechanism to focus on informative time steps.
    - Uses dense layers to perform classification over `num_labels` classes.

  Assumes the following global variables are predefined:
    - `vectorizer`: A `TextVectorization` layer.
    - `num_words`: Vocabulary size (should match vectorizer's vocab size).
    - `embedding_dim`: Dimensionality of the GloVe embeddings.
    - `embedding_matrix`: Preloaded GloVe embeddings matrix (shape: [num_words, embedding_dim]).

  Args:
      num_labels (int): Number of output classes for classification.

  Returns:
      tf.keras.Model: A compiled Keras Sequential model with LSTM and attention layers.

  Example:
      model = get_lstm_attention_glove_model(num_labels=5)
      model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
  """
  model = Sequential([
      Input(shape=(1,), dtype=tf.string, name='text'),
      vectorizer,
      Embedding(
          input_dim=num_words,
          output_dim=embedding_dim,
          weights=[embedding_matrix],
          trainable=True
          ),
      Bidirectional(LSTM(128, return_sequences=True)),
      Dropout(0.3),
      AttentionLayer(),
      Dropout(0.3),
      Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-3)),
      Dense(num_labels, activation='softmax')
  ])

  return model

"""Trains an LSTM + Attention model with pretrained GloVe embeddings on text data.

This script builds, compiles, and trains a Keras Sequential model using:
- A TextVectorization layer
- GloVe word embeddings
- LSTM with attention mechanism
- Dense classification layers
"""

with tf.device(device_name):
  glove_lstm_attention_model = get_lstm_attention_glove_model(num_classes)

  glove_lstm_attention_model.compile(
      optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, clipnorm=1.0),
      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
      metrics=['accuracy']
  )
  glove_lstm_attention_model.summary()

  history = glove_lstm_attention_model.fit(
    glove_train_ds,
    validation_data=glove_val_ds,
    class_weight=class_weight_dict,
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, verbose=1, restore_best_weights=True), tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)],
    batch_size=32,
    epochs=10
  )
  plot_history(history)
  glove_lstm_attention_model.save("lstm_attention_glove_model.keras")

"""Accuracy trend:

Training accuracy improves consistently from 17.7% → 48.5% across epochs.

Validation accuracy improves early on, from 20.8% → 29.9%.

The best validation accuracy (29.95%) occurs at Epoch 9, but it only slightly improves over values from Epochs 6–8.

The gap between training and validation accuracy begins to widen notably after Epoch 6, indicating possible overfitting.

Despite training improvement, validation accuracy plateaus around Epochs 6–9.

Loss trend:

Training loss decreases steadily from 2.20 → 1.50, reflecting good learning capacity and model fit.

Validation loss drops consistently from 2.11 → 1.93 until Epoch 6, then increases in Epochs 7–9, peaking at 2.04.

This pattern indicates that the model started overfitting after Epoch 6.

EarlyStopping correctly restored weights from Epoch 6, the point where validation loss was lowest (1.9392).
"""

files.download("lstm_attention_glove_model.keras")

"""#### GRU with Attention and GloVe Embeddings

RAM: ~7.9GB  
GPU RAM: ~0.4GB

**GloVe + GRU + Attention**

Why: GRUs are efficient for sequence modeling, but they treat all time steps equally. Adding attention allows the model to focus on the most relevant parts of a sentence.

Advantage:
- Combines global semantic knowledge (GloVe) with temporal dynamics (GRU)
- Attention boosts interpretability and model focus
- More stable than deep RNNs alone

Use Case: Useful for tasks requiring both sequence structure and context awareness (e.g., text classification, intent recognition).

"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.layers import Layer
import tensorflow as tf

class AttentionLayer(Layer):
  """A simple attention mechanism over temporal input sequences.

  This custom Keras layer computes a weighted average over the time steps
  of the input sequence, allowing the model to focus on relevant parts.

  Mechanism:
    - Applies a non-linear transformation (`tanh`) to the inputs.
    - Computes attention weights using `softmax` across time steps.
    - Multiplies weights with input values and reduces across time.

  Input shape:
      (batch_size, time_steps, hidden_size)

  Output shape:
      (batch_size, hidden_size)
  """
  def __init__(self):
    super(AttentionLayer, self).__init__()
  def call(self, inputs):
    score = tf.nn.tanh(inputs)
    weights = tf.nn.softmax(score, axis=1)
    context = weights * inputs
    return tf.reduce_sum(context, axis=1)

# Build GRU + Attention model
def get_gru_attention_model(num_labels):
  """Builds a GRU-based text classification model with an attention mechanism and GloVe embeddings.

  This model architecture includes:
    - Text vectorization for raw string input
    - GloVe-initialized embedding layer (trainable)
    - GRU layer for sequential pattern learning
    - Custom attention layer to focus on informative time steps
    - Dense layers with dropout and regularization for classification

  Assumes the following global variables are defined:
    - `vectorizer`: A TextVectorization layer for preprocessing input text.
    - `num_words`: Size of the vocabulary.
    - `embedding_dim`: Dimension of GloVe embeddings.
    - `embedding_matrix`: A NumPy array with pretrained GloVe vectors (shape: [num_words, embedding_dim]).

  Args:
      num_labels (int): Number of output classes for classification.

  Returns:
      tf.keras.Model: A compiled Keras Functional API model ready for training.

  Example:
      model = get_gru_attention_model(num_labels=9)
      model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
  """
  text_input = Input(shape=(1,), dtype=tf.string, name='text')
  x = vectorizer(text_input)

  x = Embedding(
      input_dim=num_words,
      output_dim=embedding_dim,
      weights=[embedding_matrix],
      trainable=True,
      name="embedding"
  )(x)

  x = GRU(128, return_sequences=True)(x)
  x = Dropout(0.3)(x)
  x = AttentionLayer()(x)
  x = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-3))(x)
  x = Dropout(0.2)(x)
  output = Dense(num_labels, activation='softmax')(x)

  return Model(inputs=text_input, outputs=output)

# Train the model
with tf.device(device_name):
    model = get_gru_attention_model(num_classes)

    # Phase 1: Freeze embeddings
    model.get_layer("embedding").trainable = False
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    history1 = model.fit(
    glove_train_ds,
    validation_data=glove_val_ds,
    class_weight=class_weight_dict,
    batch_size=32,
    epochs=3,
    callbacks=[
        EarlyStopping(patience=2, restore_best_weights=True, verbose=1)
    ]
)

# Unfreeze GloVe embeddings for fine-tuning
model.get_layer("embedding").trainable = True

# Re-compile with a lower learning rate
model.compile(
    optimizer=Adam(learning_rate=1e-4, clipnorm=1.0),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Fine-tune the entire model
history2 = model.fit(
    glove_train_ds,
    validation_data=glove_val_ds,
    class_weight=class_weight_dict,
    batch_size=32,
    initial_epoch=3,
    epochs=10,
    callbacks=[
        EarlyStopping(patience=3, restore_best_weights=True, verbose=1),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6, verbose=1)
    ]
)

history = merge_histories(history1, history2)
plot_history(history)

"""**Accuracy Trend:**
- Training accuracy improves steadily from ~20.1% to ~42.1%.
- Validation accuracy increases gradually from ~24.8% to ~32.0%.
- The model generalizes well with no major overfitting or instability.
- Validation accuracy flattens after Epoch 8, suggesting convergence.

**Loss Trend:**
- Training loss decreases consistently from ~2.11 to ~1.63.
- Validation loss drops from ~2.04 to ~1.88 and then stabilizes.
- The gap between training and validation loss remains small, indicating strong generalization.

**Observation:**
This is your most stable and best-performing GRU-based model. Attention improves contextual focus, and the model converges reliably with good validation performance. Additional gains could come from more epochs or regularization tuning.

"""

model.save("gru_attention_glove_model.keras")

"""#### CNN

**Why Use CNN for Text Classification?**

Efficient at Capturing Local Patterns (n-grams)
CNNs apply filters over word embeddings to capture local features, such as:
- Phrases
- Emotional cues
- Hashtag patterns

Word combinations like “so happy”, “not bad”, or “very sad”

These n-gram features are powerful indicators for emoji use.

Parallel Computation = Faster Training

Unlike RNNs/LSTMs, CNNs process input in parallel, not sequentially. This means 1. Faster training and 2. Less computational cost

Good for large-scale or real-time applications (e.g., live tweet emoji prediction)

GlobalMaxPooling Highlights Strongest Signals
CNNs typically use GlobalMaxPooling to extract the most important feature across the entire sentence.

This mechanism helps filter out noise and focus on the most relevant word or phrase for classification.
"""

from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from tensorflow.keras.models import Sequential

def get_cnn_model(num_labels):
  """Builds a 1D CNN-based text classification model using GloVe embeddings.

    This model processes raw input text through the following layers:
      - Text vectorization
      - Pretrained GloVe embeddings (non-trainable)
      - 1D convolution for n-gram feature extraction
      - Global max pooling for sequence reduction
      - Batch normalization and dropout for regularization
      - Dense layers for classification

    Assumes the following global variables are defined:
      - `vectorizer`: A fitted Keras `TextVectorization` layer.
      - `num_words`: Size of the vocabulary.
      - `embedding_dim`: Dimensionality of GloVe embeddings.
      - `embedding_matrix`: NumPy array of shape (num_words, embedding_dim).

    Args:
        num_labels (int): Number of output classes for classification.

    Returns:
        tf.keras.Model: A compiled Keras Sequential model ready for training.

    Example:
        model = get_cnn_model(num_labels=5)
        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    """
  model = Sequential([
      Input(shape=(1,), dtype=tf.string, name='text'),
      vectorizer,
      Embedding(
          input_dim=num_words,
          output_dim=embedding_dim,
          weights=[embedding_matrix],
          trainable=False
      ),
      Conv1D(128, 5, activation='relu'),
      GlobalMaxPooling1D(),
      BatchNormalization(),
      Dropout(0.4),
      Dense(64, activation="relu"),
      Dense(num_labels, activation="softmax")
  ])

  return model

"""Trains a CNN-based text classification model using pretrained GloVe embeddings.

This script builds and trains a model that uses:
  - A text vectorization layer
  - Frozen GloVe embeddings
  - 1D convolution and global max pooling
  - Dense layers for classification
"""

with tf.device(device_name):
  glove_cnn_model = get_cnn_model(num_classes)

  glove_cnn_model.compile(
      optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
      loss='sparse_categorical_crossentropy',
      metrics=['accuracy']
  )

  glove_cnn_model.summary()
  history = glove_cnn_model.fit(
    glove_train_ds,
    validation_data=glove_val_ds,
    class_weight=class_weight_dict,
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, verbose=1, restore_best_weights=True), tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)],
    batch_size=32,
    epochs=20
  )

  plot_history(history)
  glove_cnn_model.save("cnn_glove_model.keras")

"""Accuracy Trend:

- Training Accuracy rose gradually to ~33% over 20 epochs
- Validation Accuracy improved steadily and reached ~26%, showing some generalization


Loss Trend:
- Training Loss dropped from ~2.6 to ~1.85
- Validation Loss decreased at a slower rate, flattening around 2.0, suggesting the model converged but didn’t overfit
"""

files.download("cnn_glove_model.keras")

"""## Step 7 Evaluation

After training and validating our model, we now evaluate its performance on the unseen test data.
"""

from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

def bert_ds_to_numpy(ds):
  """Converts a BERT-style `tf.data.Dataset` into NumPy arrays for evaluation or prediction.

  The dataset is expected to yield batches of the form:
      ({'input_ids': ..., 'attention_mask': ...}, label)

  This function:
    - Unbatches the dataset
    - Extracts `input_ids`, `attention_mask`, and `labels`
    - Stacks them into NumPy arrays

  Args:
      ds (tf.data.Dataset): A TensorFlow dataset containing BERT-style inputs.

  Returns:
      tuple:
          - X_test (dict): A dictionary with:
              - 'input_ids' (np.ndarray): Token ID matrix (num_samples, seq_length)
              - 'attention_mask' (np.ndarray): Attention mask matrix (num_samples, seq_length)
          - y_test (np.ndarray): Array of labels (num_samples,)

  Example:
      X_test, y_test = bert_ds_to_numpy(bert_val_ds)
      predictions = model.predict(X_test)
  """
  input_ids_list = []
  attention_mask_list = []
  labels_list = []

  for batch in ds.unbatch().as_numpy_iterator():
      inputs, labels = batch
      input_ids_list.append(inputs["input_ids"])
      attention_mask_list.append(inputs["attention_mask"])
      labels_list.append(labels)

  X_test = {
      "input_ids": np.stack(input_ids_list),
      "attention_mask": np.stack(attention_mask_list)
  }
  return X_test, np.array(labels_list)


def evaluate_model(model, X_test, y_test, title, labels):
  """Evaluates a classification model and visualizes performance metrics.

  This function:
    - Runs model predictions on the test set
    - Applies softmax if logits are returned (common with BERT-style models)
    - Computes and prints evaluation metrics
    - Displays a confusion matrix heatmap
    - Prints a detailed classification report

  Args:
      model (tf.keras.Model): Trained Keras model or Hugging Face model with a `predict()` method.
      X_test (dict or np.ndarray): Input test data. For BERT-style models, this is a dict with `input_ids` and `attention_mask`.
      y_test (np.ndarray): True class labels.
      title (str): Title for the confusion matrix plot.
      labels (list of str): List of class names for display on the plot axes.

  Returns:
      None. Prints evaluation results and shows a confusion matrix.

  Example:
      evaluate_model(model, X_test, y_test, "Validation Results", ["happy", "sad", "angry"])
  """
  y_pred = model.predict(X_test)

  if hasattr(y_pred, "logits"):
      y_pred = tf.nn.softmax(y_pred.logits).numpy()

  y_pred = np.argmax(y_pred, axis=1)
  print(model.evaluate(X_test, y_test))

  cm = confusion_matrix(y_test, y_pred)
  sns.heatmap(cm, annot=True, fmt='d', cmap='BuGn', xticklabels=labels, yticklabels=labels)
  plt.xlabel('Predicted')
  plt.ylabel('Actual')
  plt.title(title)
  plt.show()

  print("Classification Report:")
  print(classification_report(y_test, y_pred))

X_test, y_test = bert_ds_to_numpy(bert_test_ds)

# Evaluate bert_fine_tune_model
evaluate_model(bert_fine_tune_model, X_test, y_test, title="Fine Tuned BERT Confusion Matrix", labels=np.unique(y_test))

"""Confusion Matrix Insights

Class 0 is well separated with most predictions correct (145/207).

Class 1, 5, 8 are commonly misclassified and confuse with each other or with class 7.

Class 7 has the highest number of false positives — it's overpredicted (especially from 2, 5, 8).

Some classes (like 4, 6) are not clearly separable — often confused with neighbors in latent space.
"""

# Evaluate bert_with_class_layer_model
evaluate_model(bert_with_class_layer_model, X_test, y_test, title="BERT with class layer Confusion Matrix", labels=np.unique(y_test))

"""Confusion Matrix Insights

Class 0 stands out as the most confidently predicted class.

Class 4 and 6 are frequently misclassified, often into 5 or 7 — suggesting semantic overlap or lack of distinctive patterns.

Classes 1 and 2 are occasionally confused with each other and with class 7.

Class 8 is overpredicted from many classes but shows slightly better recall.
"""

# Evaluate bert_bi_lstm_model
evaluate_model(bert_bi_lstm_model, X_test, y_test, title="BERT + BiLSTM Confusion Matrix", labels=np.unique(y_test))

"""Confusion Matrix Insights

The model achieves the same accuracy as other BERT-based models, but with slightly different distribution of strengths and weaknesses.

Recall for Class 6 improved significantly (0.52), indicating that BiLSTM captures sequential patterns that aid certain class distinctions.

However, Class 4 and 8 performance dropped due to confusion with multiple neighboring categories, showing that the LSTM layer might generalize too broadly for these.

BiLSTM adds temporal understanding on top of BERT embeddings, but in this case, it doesn't improve overall metrics—just redistributes prediction confidence.
"""

# Evaluate glove_bi_lstm_model
evaluate_model(glove_bi_lstm_model, test_texts, test_labels, title="GloVe Bidirectional LSTM Confusion Matrix", labels=np.unique(test_labels))

"""Confusion Matrix Insights

Improved balance: Compared to the earlier GloVe BiLSTM version, this model shows improved class balance. Most classes now have F1-scores above 0.30, with some recall values exceeding 0.50 (e.g., Class 0, 4).

Better Recall on Key Classes:

- Class 0 recall improved to 0.52.

- Class 4 has a strong recall of 0.54 — highest in this run.

Still Confused on Lower Frequency Classes: Performance remains weak on Class 8 (F1-score = 0.17) and Class 6 (F1 = 0.24), suggesting difficulty in distinguishing between minority or overlapping label semantics.

Confusion Spread: Although confusion remains broad, the dominant diagonal elements in the matrix indicate that the model has a stronger ability to latch onto core class signals.


"""

# Evaluate glove_lstm_attention_model
evaluate_model(glove_lstm_attention_model, test_texts, test_labels, title="GloVe LSTM Attention Confusion Matrix", labels=np.unique(test_labels))

"""Confusion Matrix Insight

Moderate diagonal dominance: Most classes have some correct predictions, indicating partial learning of class distinctions, though significant confusion still exists across several pairs (e.g., class 7 is often misclassified).

Class 4 Exception:

High precision (0.91) but low recall (0.36), suggesting that the model is highly confident but selective in predicting class 4.

Underperformance in Low Signal Classes:

Classes like 1, 7, and 8 have F1-scores below 0.25, reflecting difficulty in capturing their patterns or insufficient representational power.

Attention integration may have helped some temporal dependencies (e.g., class 2 F1 = 0.40), but not uniformly across all categories.
"""

# Evaluate final GRU + Attention model
evaluate_model(model, test_texts, test_labels, title="GRU + Attention Confusion Matrix", labels=np.unique(test_labels))

"""The model performs best on Class 4 and Class 0, both with recall near 50%.

Precision and recall are fairly balanced across most classes, leading to consistent macro and weighted F1-scores (~0.31–0.32).

Class 8 remains challenging with lower recall (0.17), often confused with neighboring classes.

Overall, the attention mechanism improved focus, but model accuracy (~32%) shows room for deeper sequence modeling or feature engineering.
"""

# Evaluate glove_cnn_model
evaluate_model(glove_cnn_model, test_texts, test_labels, title="GloVe CNN Confusion Matrix", labels=np.unique(test_labels))

"""This model performs significantly worse than LSTM- and BERT-based models.

Although some individual classes (like class 2 and 3) show slightly higher recall, most classes are poorly distinguished.

High confusion exists across nearly all class pairs — especially class 0 and 4, which are frequently misclassified as class 4 and 6.

###  Analysis Summary

**Best Performing Family:**  
BERT-based models achieved the highest overall performance (accuracy ~38%) with more balanced precision/recall across most classes. Among them:
- **BERT + BiLSTM** had the best F1 consistency.
- **BERT with custom classifier head** was also strong, with slightly better class-wise separation for class 0 and 2.

**GloVe-Based Models:**
- **GRU + Attention** outperformed other GloVe models with 32% accuracy and F1 stability across several classes. Attention helped focus on discriminative words.
- **BiLSTM (GloVe)** was next best (33%), benefiting from sequential modeling.
- **LSTM + Attention** showed good per-class focus (especially for class 3 and 5), but was slightly less stable overall.

**Underperforming Model:**
- **GloVe CNN** had the lowest accuracy (24%), struggling with context and overlapping class features. While CNNs work well on short/local structures, this task benefits more from sequential context modeling (RNNs or Transformers).

**Conclusion:**
For this multi-class tweet-emoji prediction task, **BERT-based architectures clearly generalize better**. However, a well-tuned GRU + Attention model with GloVe offers a strong lightweight alternative.

### Future Work

1. Explore alternative pre-trained transformers:
  - Incorporate models like BERTweet, RoBERTa, or Emoji-Enhanced BERT (EEBERT) that are pre-trained - on social media or emoji-rich data for improved contextual understanding.

2. Leverage emoji semantics:
  - Integrate external emoji knowledge (e.g., emoji2vec or EmojiNet) to improve class separability, especially for emojis with similar sentiment or visual appearance.

3. Develop hybrid model architectures:
  - Combine CNNs, BiLSTM, and attention mechanisms to capture both local and long-range dependencies in text more effectively.

4. Extend to emoji position prediction:
  - Transform the task into a sequence tagging problem to not only classify emojis but also predict their position within the tweet.

5. Optimize for deployment:
  - Investigate lightweight transformer variants (e.g., DistilBERT, ALBERT) and apply compression techniques (e.g., pruning, quantization, distillation) for real-time applications.

## Step 8 Interative Demo (Streamlit)
"""

!pip install -q streamlit plotly

"""### Model loading and inference test

The following functions are testing the feasibility of loading a model from weights, keras zips before integrating into the Streamlit app.
"""

import numpy as np
import pandas as pd
import tensorflow as tf

def load_model(model_path: str):
  """Loads a Keras model from a given file path.

  This function loads a previously saved Keras model in `.keras` or `.h5` format.

  Args:
      model_path (str): Path to the saved model file.

  Returns:
      tf.keras.Model: The loaded Keras model instance.

  Example:
      model = load_model("cnn_glove_model.keras")
  """
  return tf.keras.models.load_model(model_path)

def load_preprocessor(preprocessor_path: str):
  """Loads a saved text preprocessor from a file using pickle.

  This function is typically used to load objects like `TextVectorization` layers,
  tokenizers, or custom preprocessing classes that were serialized with `pickle`.

  Args:
      preprocessor_path (str): Path to the `.pkl` file containing the serialized preprocessor.

  Returns:
      Any: The deserialized preprocessor object.

  Example:
      preprocessor = load_preprocessor("bert_text_processor.pkl")
      processed_ds = preprocessor("Hello world!")
  """
  with open(preprocessor_path, 'rb') as f:
    return pickle.load(f)

def load_mapping(mapping_path: str):
  """Loads an emoji-to-label mapping from a CSV file.

  The CSV file is expected to have at least two columns: one for emojis and one for numeric labels.
  This function loads only the necessary columns and renames them to "Emoji" and "Label".

  Args:
      mapping_path (str): Path to the CSV file containing emoji and label mappings.

  Returns:
      pd.DataFrame: A DataFrame with columns:
          - "Emoji": Emoji characters (str)
          - "Label": Corresponding class labels (int)

  Example:
      mapping_df = load_mapping("Mapping.csv")
      print(mapping_df.head())
  """
  return pd.read_csv(mapping_path, usecols=[1, 2], names=["Emoji", "Label"], header=0)

def predict_emoji(tweet: str, model_path="simple-lstm.keras", preprocessor_path="preprocessor.pkl", mapping_path="Mapping.csv") -> pd.DataFrame:
  """Predicts the most likely emojis for a given tweet using a saved Keras model.

  This function loads:
    - A trained emoji classification model (e.g., LSTM or CNN with GloVe)
    - A text preprocessor (e.g., TextVectorization or custom processor)
    - An emoji-label mapping from a CSV file

  It processes the tweet, performs prediction, and returns a ranked list of emojis.

  Args:
      tweet (str): The input tweet or text string to classify.
      model_path (str, optional): Path to the saved Keras model (.keras or .h5). Defaults to "simple-lstm.keras".
      preprocessor_path (str, optional): Path to the serialized preprocessor (e.g., .pkl). Defaults to "preprocessor.pkl".
      mapping_path (str, optional): Path to the emoji-label mapping CSV file. Defaults to "Mapping.csv".

  Returns:
      pd.DataFrame: A DataFrame with two columns:
          - "emoji": Emoji characters (str)
          - "prob": Model-predicted probabilities (float)

  TODO:
      - Add functionality to predict the most appropriate position in the tweet for emoji insertion.

  Example:
      >>> predict_emoji("happy happy happy", "model.keras", "text_processor.pkl", "Mapping.csv")
  """

  # return pd.DataFrame({"emoji": ["😂","😃","😊","😍","😣"], "prob":[0.876, 0.11, 0.04, 0.02, 0.01]})

  target_labels = [9, 2, 3, 7, 15, 13, 16, 17, 1]

  preprocessor = load_preprocessor(preprocessor_path)
  model = load_model(model_path)
  mapping = load_mapping(mapping_path)

  mapping = mapping[mapping["Label"].isin(target_labels)].sort_values("Label").reset_index(drop=True)

  inference_ds = preprocessor(tweet)

  pred = model.predict(inference_ds.batch(1))[0]

  return pd.DataFrame({"emoji": mapping["Emoji"].to_list(), "prob": pred})

predict_emoji("happy happy happy", "bidirectional_lstm_glove_model.keras", "glove_text_processor.pkl", "Mapping.csv").sort_values('prob', ascending=False).head(10)

from transformers import TFBertForSequenceClassification

def load_bert_model(model_name: str):
  """Loads a pretrained or fine-tuned BERT-based classification model by name.

  Depending on the provided `model_name`, this function loads one of the following:
    - A BiLSTM-BERT hybrid model with weights.
    - A BERT model with a custom classifier head and weights.
    - A Hugging Face fine-tuned BERT model from a saved directory.

  Supported model names:
    - "bidirectional_lstm": Loads a BiLSTM + BERT model with pretrained weights.
    - "bert_with_classifier": Loads a BERT + dense classifier model with pretrained weights.
    - "fine_tuned_bert": Loads a Hugging Face model from a directory named 'fine_tuned_bert'.

  Note:
      This function assumes weights for custom models are stored in:
      - 'bidirectional_lstm_bert_model.weights.h5'
      - 'bert_with_class_layer_model.weights.h5'

  Args:
      model_name (str): The name of the model to load.

  Returns:
      tf.keras.Model or transformers.TFBertForSequenceClassification: The loaded model.

  Raises:
      ValueError: If the provided model name is unsupported.

  Example:
      model = load_bert_model("fine_tuned_bert")
  """
  if model_name == "bidirectional_lstm":
    model = get_bidirectional_lstm_bert_model(9)
    model.load_weights('bidirectional_lstm_bert_model.weights.h5')
    return model

  if model_name == "bert_with_classifier":
    model = get_bert_with_class_layer_model(9)
    model.load_weights('bert_with_class_layer_model.weights.h5')
    return model

  if model_name == "fine_tuned_bert":
    model = TFBertForSequenceClassification.from_pretrained('fine_tuned_bert', num_labels=9)
    return model

def predict_emoji_bert(tweet: str, model_name, preprocessor_path, mapping_path="Mapping.csv") -> pd.DataFrame:
  """Predicts the most likely emojis for a given tweet using a BERT-based classifier.

  This function:
    - Loads a pre-trained BERT model.
    - Preprocesses the input tweet.
    - Predicts emoji probabilities.
    - Maps numeric labels to emoji characters using a CSV mapping file.

  Currently supports classification into a fixed set of 9 emojis.

  Args:
      tweet (str): The input tweet string.
      model_name (str): Name of the model to load (e.g., "bert_with_classifier", "fine_tuned_bert").
      preprocessor_path (str): Path to the saved text preprocessor (e.g., a pickled `TextProcessor` or tokenizer).
      mapping_path (str, optional): Path to the emoji label mapping CSV. Defaults to "Mapping.csv".

  Returns:
      pd.DataFrame: A DataFrame with two columns:
          - 'emoji': Emoji character (str)
          - 'prob': Predicted probability (float) for each emoji

  Raises:
      ValueError: If the model name is unsupported.

  TODO:
      - Predict the most likely position in the tweet to place the predicted emoji(s).

  Example:
      >>> predict_emoji_bert("I love you", "bert_with_classifier", "bert_text_processor.pkl")
  """

  target_labels = [9, 2, 3, 7, 15, 13, 16, 17, 1]

  preprocessor = load_preprocessor(preprocessor_path)
  model = load_bert_model(model_name)
  mapping = load_mapping(mapping_path)

  mapping = mapping[mapping["Label"].isin(target_labels)].sort_values("Label").reset_index(drop=True)

  inference_ds = preprocessor(tweet)
  pred = model.predict(inference_ds.batch(1))[0].flatten()

  if model_name == "fine_tuned_bert":
      pred = tf.nn.softmax(pred).numpy()

  return pd.DataFrame({"emoji": mapping["Emoji"].to_list(), "prob": pred})

predict_emoji_bert("I love you", "bert_with_classifier", "bert_text_processor.pkl").sort_values('prob', ascending=False).head(10)

!unzip fine_tuned_bert.zip -d fine_tuned_bert

"""### Build application resources and execution script"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile models.py
# 
# from transformers import TFBertModel
# from tensorflow.keras.layers import Input, Dense, Dropout, Lambda, BatchNormalization, Bidirectional, LSTM, Layer
# from tensorflow.keras.models import Model
# import re
# import tensorflow as tf
# 
# MAX_LEN = 100
# 
# class BertCLS(Layer):
#     def __init__(self, bert_model, **kwargs):
#         super().__init__(**kwargs)
#         self.bert = bert_model
# 
#     def call(self, inputs):
#         input_ids, attention_mask = inputs
#         outputs = self.bert(input_ids, attention_mask=attention_mask)
#         return outputs.last_hidden_state[:, 0, :]  # [CLS] token output
# 
# def get_bert_with_class_layer_model(num_classes):
#   bert = TFBertModel.from_pretrained("bert-base-uncased")
#   input_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_ids')
#   attention_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name='attention_mask')
# 
#   x = BertCLS(bert)([input_ids, attention_mask])
# 
#   x = Dropout(0.3)(x)
#   x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)
#   x = BatchNormalization()(x)
#   x = Dropout(0.3)(x)
#   x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)
#   x = BatchNormalization()(x)
#   x = Dense(128, activation='relu')(x)
#   x = Dropout(0.3)(x)
# 
#   output = Dense(num_classes, activation='softmax')(x)
# 
#   model = Model(inputs=[input_ids, attention_mask], outputs=output)
#   return model
# 
# class Bert(Layer):
#     def __init__(self, bert_model, **kwargs):
#         super().__init__(name="bert_sequence_output", **kwargs)
#         self.bert = bert_model
# 
#     def call(self, inputs):
#         input_ids, attention_mask = inputs
#         outputs = self.bert(input_ids, attention_mask=attention_mask)
#         return outputs.last_hidden_state
# 
# def get_bidirectional_lstm_bert_model(num_labels):
#   # Load base BERT model (no classification head)
#   bert_model = TFBertModel.from_pretrained('bert-base-uncased')
#   bert_model.trainable = False
# 
#   # Input layers
#   input_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_ids')
#   attention_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name='attention_mask')
# 
#   x = Bert(bert_model)([input_ids, attention_mask])  # shape: (batch_size, MAX_LEN, 768)
# 
#   x = Bidirectional(LSTM(128, return_sequences=False))(x)  # shape: (batch_size, 256)
# 
#   # Dense + Dropout + BatchNorm head
#   x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)
#   x = BatchNormalization()(x)
#   x = Dropout(0.3)(x)
# 
#   x = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)
#   x = BatchNormalization()(x)
#   x = Dropout(0.3)(x)
# 
#   output = Dense(num_labels, activation='softmax')(x)
# 
#   return Model(inputs=[input_ids, attention_mask], outputs=output)
# 
# class GloveTextProcessor:
#   def clean_text(self, text):
#     text = re.sub(r"@\w+", "", text)
#     text = re.sub(r"http\S+", "", text)
#     text = re.sub(r"#\w+", "", text)
#     return text.lower().strip()
# 
#   def __call__(self, text):
#       cleaned_text = self.clean_text(text)
#       return tf.data.Dataset.from_tensor_slices(tf.constant([cleaned_text], dtype=tf.string))
# 
# class TextProcessor:
#     def __init__(self, tokenizer):
#         self.tokenizer = tokenizer
# 
#     def clean_text(self, text):
#       text = re.sub(r"@\w+", "", text)
#       text = re.sub(r"http\S+", "", text)
#       text = re.sub(r"#\w+", "", text)
#       return text.lower().strip()
# 
#     def __call__(self, text, max_len=100):
#         cleaned_text = self.clean_text(text)
#         encodings = self.tokenizer(
#             [cleaned_text],  # Wrap in a list as tokenizer expects list of strings
#             truncation=True,
#             padding='max_length',
#             max_length=max_len,
#             return_tensors='tf'
#         )
# 
#         return tf.data.Dataset.from_tensor_slices({
#             "input_ids": encodings["input_ids"],
#             "attention_mask": encodings["attention_mask"]
#           })
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import plotly.express as px
# import tensorflow as tf
# from transformers import TFBertForSequenceClassification
# import re
# import pickle
# from models import get_bert_with_class_layer_model, get_bidirectional_lstm_bert_model, GloveTextProcessor, TextProcessor
# 
# prev_tweet = ""
# 
# target_labels = [9, 2, 3, 7, 15, 13, 16, 17, 1]
# 
# @st.cache_resource
# def load_model(model_path: str):
#   return tf.keras.models.load_model(model_path)
# 
# @st.cache_resource
# def load_preprocessor(preprocessor_path: str):
#   with open(preprocessor_path, 'rb') as f:
#     return pickle.load(f)
# 
# @st.cache_resource
# def load_mapping(mapping_path: str):
#   return pd.read_csv(mapping_path, usecols=[1, 2], names=["Emoji", "Label"], header=0)
# 
# @st.cache_resource
# def load_bert_model(model_name: str):
#   if model_name == "bidirectional_lstm":
#     model = get_bidirectional_lstm_bert_model(len(target_labels))
#     model.load_weights('bidirectional_lstm_bert_model.weights.h5')
#     return model
# 
#   if model_name == "bert_with_classifier":
#     model = get_bert_with_class_layer_model(len(target_labels))
#     model.load_weights('bert_with_class_layer_model.weights.h5')
#     return model
# 
#   if model_name == "fine_tuned_bert":
#     model = TFBertForSequenceClassification.from_pretrained('fine_tuned_bert', num_labels=len(target_labels))
#     return model
# 
# 
# def predict_emoji_bert(tweet: str, model_name, preprocessor_path="bert_text_processor.pkl", mapping_path="Mapping.csv") -> pd.DataFrame:
#   preprocessor = load_preprocessor(preprocessor_path)
#   model = load_bert_model(model_name)
#   mapping = load_mapping(mapping_path)
# 
#   mapping = mapping[mapping["Label"].isin(target_labels)].sort_values("Label").reset_index(drop=True)
# 
#   inference_ds = preprocessor(tweet)
#   pred = model.predict(inference_ds.batch(1))[0].flatten()
# 
#   if model_name == "fine_tuned_bert":
#       pred = tf.nn.softmax(pred).numpy()
# 
#   return pd.DataFrame({"emoji": mapping["Emoji"].to_list(), "prob": pred})
# 
# 
# 
# def predict_emoji(tweet: str, model_path="cnn_glove_model.keras", preprocessor_path="glove_text_processor.pkl", mapping_path="Mapping.csv") -> pd.DataFrame:
#   """
#   TODO: also predict the position of emoji in the tweet
#   """
# 
#   # return pd.DataFrame({"emoji": ["😂","😃","😊","😍","😣"], "prob":[0.876, 0.11, 0.04, 0.02, 0.01]})
# 
#   preprocessor = load_preprocessor(preprocessor_path)
#   model = load_model(model_path)
#   mapping = load_mapping(mapping_path)
# 
#   mapping = mapping[mapping["Label"].isin(target_labels)].sort_values("Label").reset_index(drop=True)
# 
#   inference_ds = preprocessor(tweet)
#   pred = model.predict(inference_ds.batch(1))[0]
# 
#   return pd.DataFrame({"emoji": mapping["Emoji"].to_list(), "prob": pred})
# 
# 
# def plot_emoji_bar_chart(df: pd.DataFrame):
#   df_sorted = df.sort_values('prob', ascending=False).head(5)
# 
#   df_sorted["emoji"] = pd.Categorical(df_sorted["emoji"], categories=df_sorted["emoji"], ordered=True)
# 
#   # Plot using Plotly Express
#   fig = px.bar(
#       df_sorted,
#       x='prob',
#       y='emoji',
#       orientation='h',
#       labels={'prob': 'Probability', 'emoji': 'Emoji'},
#       title='Top 5 Predicted Emoji'
#   )
# 
#   fig.update_layout(yaxis=dict(autorange="reversed"))
# 
#   st.plotly_chart(fig)
# 
# def clear_text():
#   st.session_state["text"] = ''
# 
# # Page title
# st.set_page_config(page_title='🕊 Tweet Emoji Prediction | Group 1')
# st.title('🕊 Tweet Emoji Prediction | Group 1')
# 
# st.markdown("""
# ## Group Members:
# 
# - Avinash Sudireddy
# - Ishika Fatwani
# - Lily Gharacheh
# - Lok Yin Wong
# - Safoora Akrami
# - Shashi Singh
# - Vrushank Sharma
# """)
# 
# tweet = st.text_input("Enter a tweet:", "", key="text")
# col1, col2 = st.columns([3, 1])
# 
# with col1:
#   model_name = st.selectbox(
#       "Choose a model:",
#       ("Fine-tuned BERT", "BERT + Classifier", "BERT + BiLSTM", "gloVe + BiLSTM", "gloVe + LSTM + Attention", "gloVe + GRU", "gloVe + CNN")
#   )
# 
# with col2:
#   st.markdown("""
#         <style>
#         #spacer {
#             height: 12px;
#         }
#         </style>
#         <p id='spacer'></p>
#     """, unsafe_allow_html=True)
#   isPredict = st.button("Predict", type="primary", use_container_width=True)
# 
# if (tweet != prev_tweet) or isPredict:
#   if tweet != prev_tweet:
#     prev_tweet = tweet
# 
#     if model_name == "Fine-tuned BERT":
#         emojis = predict_emoji_bert(tweet, model_name="fine_tuned_bert")
#     elif model_name == "BERT + Classifier":
#         emojis = predict_emoji_bert(tweet, model_name="bert_with_classifier")
#     elif model_name == "BERT + BiLSTM":
#         emojis = predict_emoji_bert(tweet, model_name="bidirectional_lstm")
#     elif model_name == "gloVe + BiLSTM":
#       emojis = predict_emoji(tweet, model_path="bidirectional_lstm_glove_model.keras")
#     elif model_name == "gloVe + LSTM + Attention":
#       emojis = predict_emoji(tweet, model_path="lstm_attention_glove_model.keras")
#     elif model_name == "gloVe + GRU":
#       emojis = predict_emoji(tweet, model_path="gru_glove_model.keras")
#     elif model_name == "gloVe + CNN":
#       emojis = predict_emoji(tweet, model_path="cnn_glove_model.keras")
# 
#     top_emoji = emojis.iloc[int(emojis["prob"].idxmax())]["emoji"]
# 
#     plot_emoji_bar_chart(emojis)
#     st.write("Tweet:", f"{tweet}{top_emoji}")
#     st.write("Predicted emoji:", top_emoji)

!curl https://loca.lt/mytunnelpassword

!streamlit run app.py &>/content/logs.txt &
!npx localtunnel --port 8501